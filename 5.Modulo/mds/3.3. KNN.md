## K Nearest Neighbors (KNN)  

K Nearest Neighbor (KNN) es un algoritmo de aprendizaje automático muy simple, fácil de entender y versátil. Se utiliza en muchas áreas diferentes, como la detección de escritura a mano, el reconocimiento de imágenes y el reconocimiento de video. KNN es más útil cuando los datos etiquetados son demasiado costosos o imposibles de obtener, y puede lograr una alta precisión en una amplia variedad de problemas de predicción.

KNN es un algoritmo simple, basado en el mínimo local de la función objetivo que se utiliza para aprender una función desconocida de precisión y exactitud deseadas. El algoritmo también encuentra la vecindad de una entrada desconocida, su rango o distancia de ella y otros parámetros. Se basa en el principio de "ganancia de información": el algoritmo descubre cuál es el más adecuado para predecir un valor desconocido.

### Lazy learning

KNN es ampliamente conocido como un algoritmo ML que no necesita ningún entrenamiento en datos. Esto es muy diferente de los enfoques de aprendizaje early que se basan en un conjunto de datos de entrenamiento para realizar predicciones sobre datos no vistos. Con KNN, no necesita una fase de entrenamiento.

KNN se basa en similitudes de datos observables y métricas de distancia sofisticadas para generar predicciones precisas. Esta técnica puede parecer un poco contraria a la intuición pero en realidad es muy confiable. Es popular en muchos campos, incluyendo:

* Visión por computador: KNN realiza tareas de clasificación. Maneja bien los datos de la imagen y se considera una buena opción para clasificar un montón de imágenes diversas en función de las similitudes.

* Recomendación de contenido: KNN es excelente para la recomendación de contenido. Se utiliza en muchos motores de sistemas de recomendación y sigue siendo relevante a pesar de que ya hay disponibles sistemas más nuevos y potentes.

### Dimensionalidad

La maldición de la dimensionalidad significa que KNN funciona mejor con un número reducido de features. Cuando aumenta el número de features, se requieren más datos. 

Cuando hay más datos, se puede crear un problema de overfitting porque nadie sabe qué pieza de ruido contribuirá al modelo. KNN funciona mejor con baja dimensionalidad.

### Funcionamiento

Para una observación que no está en el conjunto de datos, el algoritmo simplemente buscará el número K de instancias definidas como similares en función del perímetro más cercano a esa observación. 

Cualquier punto de datos cae dentro de un grupo específico si está lo suficientemente cerca de él.

Para K vecinos, el algoritmo usará su salida para calcular la variable y de la observación que queremos predecir.

Como tal:

* Si se usa KNN para tareas de regresión, las predicciones se basarán en la media o mediana de las K observaciones más cercanas.
* Si se utiliza KNN con fines de clasificación, la moda de las observaciones más cercanas servirá para la predicción.

Supongamos que tenemos:

* un conjunto de datos D
* una métrica de distancia definida que usaremos para medir la distancia entre el conjunto de observaciones
* y un número entero K que representa el número mínimo de vecinos cercanos que debemos considerar para establecer la proximidad

Para poder predecir la salida y para una nueva observación X, se seguirán estos pasos:

1. Calcule las distancias totales entre el observable X y todos los puntos de datos.
2. Conservar las K observaciones que constituyen las distancias menores al punto observable X.
3. Con las salidas y tomadas de las observaciones K:
	1. Regresión: aplicar la media de las deducciones "y" .
	2. Clasificación: usar la moda de las deducciones "y".
4. La predicción final será el valor calculado en el paso 3.

### Introducción a KNN

Lo primero que se debe saber sobre el funcionamiento interno de kNN es que es un algoritmo perezoso o lazy. En la jerga del aprendizaje automático, significa que el algoritmo no tiene una fase de entrenamiento o es extremadamente corta en comparación con otros. 

La generación de predicciones será mucho más lenta debido a cómo kNN encuentra a los vecinos más cercanos. En la breve fase de entrenamiento, memoriza todos los puntos de datos. 

Para hacer una predicción, el algoritmo encuentra la distancia entre la nueva muestra y cada punto de datos en el conjunto de datos. Luego, toma las k distancias más pequeñas y clasifica la nueva muestra en función de la mayoría del voto de sus vecinos.

Para el ojo humano, etiquetar el punto verde no es ningún problema. Pero, no hay forma de que el algoritmo encuentre los vecinos más cercanos sin calcular la distancia entre cada punto.

La distancia en sí se calcula utilizando una de las 3 métricas de distancia:

* Distancia **euclidiana**: la raíz cuadrada de la suma de las diferencias entre los componentes de xey.

* Distancia de **Manhattan**: la suma de los valores absolutos de las diferencias entre los componentes de las coordenadas x e y.

* La distancia de **Minkowski** es la generalización de las dos técnicas anteriores. Tiene un parámetro p que toma un valor de 1 o 2. Cuando p=1, la fórmula se convierte en distancia manhattan yp=2 para euclidiana.


La falta de fase de entrenamiento también significa que el algoritmo no generaliza.

**KNN** también es un algoritmo no paramétrico: no tiene requisitos estrictos sobre la forma y distribución de sus datos. A diferencia de la **regresión lineal**, que supone que sus características y el objetivo tienen una relación lineal, kNN no hace tales suposiciones. 

Es por eso que se considera uno de los modelos más fáciles que existen y funciona bastante bien desde el primer momento si se entrena con los datos "apropiados". 

### KNN en scikit learn

[KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html  )
  
[KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html  )
  
Normalmente lo utilizamos para clasificación pero también se puede usar en regresión.
  
Importante: No confundir con el algoritmo K-means el cual es de aprendizaje no supervisado.  
  
Idea: asigna una etiqueta a nuevos datos de entrada basándose en la distancia entre esos nuevos datos y los antiguos.  
  
K vecinos más cercanos.  
  
El parámetro K:  
- Elegir K impar  
- En caso de que para un K haya empate reducimos el K hasta que desempate  
- En caso de tener dos vecinos k+1 y k con idénticas distancias pero diferentes etiquetas, entonces el resultado dependerá del orden de los datos de entrenamiento  
  
Algoritmo KNN:  
- Elige K  
- Ordenar los vectores de features por distancia  
- Elige la clase basándose en el K vector de features más cercano.  
- Mide la distancia utilizando una de las siguientes fórmulas:  
* Minkowski  
* Manhattan  
* ...  
* https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric  
  
Recomendaciones:  
  
- Escalar los datos de las features dado que pueden tener rangos de valores muy distintos y eso puede afectar a cómo se mide la distancia.  
  

### Cómo elegir el valor K:  
  
* Buscamos un K que minimice el error: Error = 1 - accuracy  
* 2 métodos:  
	* El método del codo: se pinta en una gráfica el ratio de error para cada k y se puede observar a partir de cierto k la mejora empieza a ser        insignificante.  
	* GridSearchCV  
* A mayor de K, mayor complejidad en el modelo por tanto la idea no es buscar un k elevado, si no un k con un bajo error.


### Ventajas


KNN en scikit learn:
* Simple y fácil de implementar. 
* Fase de entrenamiento rápida debido a que es un lazy learner: El aprendiz perezoso hace que el KNN sea mucho más rápido que el otro algoritmo. 
* Sin suposición de la distribución subyacente de los datos
* Versátil: se puede utilizar tanto para la regresión como para la clasificación

KNN es un algoritmo bastante simple de entender. No se basa en ningún modelo de ML que funcione internamente y haga predicciones. KNN es un algoritmo de clasificación que solo necesita saber el número de categorías (una o más). Esto significa que puede determinar fácilmente si se debe agregar una nueva categoría sin ningún dato sobre cuántas otras categorías puede haber.

### Desventajas

* En el algoritmo KNN, siempre calculamos el valor de K, lo que puede ser difícil algunas veces. 
* En KNN, el costo de la predicción para el gran conjunto de datos es alto porque el costo de calcular la distancia entre los nuevos datos y los datos emocionantes es alto. 
* KNN es muy sensible a los conjuntos de datos ruidosos.
*  En KNN necesitamos escalar características antes de aplicar el algoritmo KNN a cualquier conjunto de datos. 
* KNN no funciona con características categóricas porque es difícil encontrar la distancia de las variables categóricas.
* Funciona lentamente para conjuntos de datos muy grandes: Fase de predicción muy lenta
* Incapacidad para manejar y representar muchas características
* Gran uso de memoria porque almacena todos los datos
* Sensible a las magnitudes: es por eso que las características necesarias deben escalarse 
* Sensible a los valores atípicos y al ruido. 
* Sufre de la maldición de la dimensionalidad (curse of dimensionality)

La desventaja de esta simplicidad es que no hace predicciones para cosas raras (como nuevas enfermedades), donde KNN no puede predecir porque no tiene idea de cuál sería la prevalencia de una cosa rara en una población sana. 

Aunque KNN produce una buena precisión en el conjunto de prueba, el clasificador sigue siendo más lento y costoso en términos de tiempo y memoria. Requiere una gran cantidad de memoria para almacenar todo el conjunto de datos de entrenamiento para la predicción. 

Además, la distancia euclidiana es muy sensible a las magnitudes, por lo que las características del conjunto de datos que tienen magnitudes altas siempre pesarán más que sus contrapartes con magnitudes bajas. 

Finalmente, debemos tener en cuenta que KNN no es adecuado para conjuntos de datos de gran dimensión con todo lo que hemos mencionado anteriormente.

### Conclusión
KNN es un algoritmo simple y fácil de implementar, especialmente si tenemos un pequeño conjunto de características para analizar y si nuestros datos están estratificados en un número fijo de clases. 