# Clasificación
![[classifiers.png]]
  


https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html

La **clasificación** es una tarea en la que un algoritmo de aprendizaje automático aprende a asignar una etiqueta de clase a ejemplos del dominio del problema. Ejemplo: clasificar los correos electrónicos como "spam" o "no spam".

Ejemplos de problemas de clasificación incluyen:

* Dado un carácter escrito a mano, clasificarlo como uno de los caracteres conocidos.
* Churn prediction. ¿Se va a ir un cliente?
* ¿ Es este prospecto un cliente ? ¿Compra o no compra?
* Para emitir un seguro, ¿debe realizar esta persona un análisis médico primero?
* ¿Renovará este cliente el seguro?
* ¿Fallará este equipo industrial?

Desde una perspectiva de modelado, se trata de un problema de aprendizaje supervisado, la clasificación requiere un conjunto de datos de entrenamiento con muchos ejemplos de entradas y salidas de las cuales aprender.

Un modelo utilizará el conjunto de datos de entrenamiento y calculará la mejor manera de asignar ejemplos de datos de entrada a etiquetas de clase específicas. Como tal, el conjunto de datos de entrenamiento debe ser lo suficientemente representativo del problema y tener muchos ejemplos de cada etiqueta de clase.

Las etiquetas de clase suelen ser valores de cadena, p. "spam", "no spam" y debe asignarse a valores numéricos antes de proporcionarse a un algoritmo para el modelado. Esto a menudo se denomina codificación de etiquetas, donde se asigna un número entero único a cada etiqueta de clase, p. “spam” = 0, “sin spam” = 1.

Hay muchos tipos diferentes de algoritmos de clasificación para modelar problemas de modelado predictivo de clasificación. Por ejemplo en scikit-learn: LogisticRegression, KNeighborsClassifier, SVC, DecisionTreeClassifier, etc.

Los algoritmos de modelado predictivo de clasificación se evalúan en función de sus resultados. La precisión de la clasificación es una métrica popular que se utiliza para evaluar el rendimiento de un modelo en función de las etiquetas de clase predichas. La precisión de la clasificación no es perfecta, pero es un buen punto de partida para muchas tareas de clasificación.

En lugar de etiquetas de clase, algunas tareas pueden requerir la predicción de una probabilidad de pertenencia a una clase para cada ejemplo. Esto proporciona una incertidumbre adicional en la predicción que una aplicación o un usuario pueden interpretar. Un diagnóstico popular para evaluar las probabilidades previstas es la curva ROC.

Hay quizás cuatro tipos principales de tareas de clasificación que puede encontrar; ellos son:

1. Clasificación binaria
2. Clasificación multi-clase
3. Clasificación multi-etiqueta
4. Clasificación desbalanceada/desequilibrada


## Tipos de clasificación

### 1. Clasificación binaria

La **clasificación binaria** se refiere a aquellas tareas de clasificación que tienen dos etiquetas de clase.

Ejemplos incluyen:

* Detección de correo no deseado (spam o no).
* Predicción de churn (churn o no).
* Predicción de conversión (comprar o no).

Por lo general, las tareas de clasificación binaria involucran una clase que es el estado normal y otra clase que es el estado anormal.

Por ejemplo, "no spam" es el estado normal y "spam" es el estado anormal. Otro ejemplo es “cáncer no detectado” es el estado normal de una tarea que involucra un examen médico y “cáncer detectado” es el estado anormal.

A la clase para el estado normal se le asigna la etiqueta de clase 0 y a la clase con el estado anormal se le asigna la etiqueta de clase 1.

Es común modelar una tarea de clasificación binaria con un modelo que predice una distribución de probabilidad de Bernoulli para cada ejemplo.

La distribución de Bernoulli es una distribución de probabilidad discreta que cubre un caso en el que un evento tendrá un resultado binario como 0 o 1. Para la clasificación, esto significa que el modelo predice una probabilidad de que un ejemplo pertenezca a la clase 1, o el estado anormal, 0.

Los algoritmos populares que se pueden usar para la clasificación binaria incluyen:

* Regresión logística
* k-vecinos más cercanos (KNN)
* Árboles de decisión
* Máquinas de vectores soporte (SVM)
* Naive Bayes

Algunos algoritmos están diseñados específicamente para la clasificación binaria y no admiten de forma nativa más de dos clases; los ejemplos incluyen regresión logística y máquinas de vectores de soporte.

### 2. Clasificación multiclase

https://scikit-learn.org/stable/modules/multiclass.html#multiclass-classification

La **clasificación multiclase** se refiere a aquellas tareas de clasificación que tienen más de dos etiquetas de clase.

Ejemplos incluyen:

* Clasificación de rostros.
* Clasificación de especies de plantas.
* Reconocimiento óptico de caracteres.

A diferencia de la clasificación binaria, la clasificación multiclase no tiene la noción de resultados normales y anormales. En su lugar, los ejemplos se clasifican como pertenecientes a una entre una gama de clases conocidas.

El número de etiquetas de clase puede ser muy grande en algunos problemas. Por ejemplo, un modelo puede predecir que una foto pertenece a uno entre miles o decenas de miles de rostros en un sistema de reconocimiento facial.

Los problemas que involucran la predicción de una secuencia de palabras, como los modelos de traducción de texto, también pueden considerarse un tipo especial de clasificación multiclase. Cada palabra en la secuencia de palabras a predecir implica una clasificación de múltiples clases donde el tamaño del vocabulario define el número de clases posibles que se pueden predecir y podría tener un tamaño de decenas o cientos de miles de palabras.

Es común modelar una tarea de clasificación multiclase con un modelo que predice una distribución de probabilidad de Multinoulli para cada ejemplo.

La distribución de Multinoulli es una distribución de probabilidad discreta que cubre un caso en el que un evento tendrá un resultado categórico, p. K en {1, 2, 3, …, K}. Para la clasificación, esto significa que el modelo predice la probabilidad de que un ejemplo pertenezca a cada etiqueta de clase.

Muchos algoritmos utilizados para la clasificación binaria se pueden utilizar para la clasificación multiclase.

Los algoritmos populares que se pueden usar para la clasificación de múltiples clases incluyen:

* K-Vecinos más cercanos (KNN)
* Árboles de decisión
* Naive Bayes
* Random Forest
* Gradient Boosting

Los algoritmos que están diseñados para la clasificación binaria se pueden adaptar para su uso en problemas de clases múltiples.

Esto implica el uso de una estrategia de ajuste de múltiples modelos de clasificación binaria para cada clase frente a todas las demás clases (llamado one-vs-rest) o un modelo para cada par de clases (llamado one-vs-one).

* **one-vs-rest**: ajuste un modelo de clasificación binaria para cada clase frente a todas las demás clases.
* **one-vs-one**: ajuste un modelo de clasificación binaria para cada par de clases.

Los algoritmos de clasificación binaria que pueden usar estas estrategias para la clasificación de clases múltiples incluyen:

 * Regresión logística.
 * Máquinas de vectores soporte.

### 3. Clasificación multi etiqueta

https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification

La **clasificación multi etiqueta** se refiere a aquellas tareas de clasificación que tienen dos o más etiquetas de clase, donde se pueden predecir una o más etiquetas de clase para cada ejemplo. Es decir, se puede asignar más de una clase a un dato.

El ejemplo de la clasificación de fotografías, donde una determinada fotografía puede tener múltiples objetos en la escena y un modelo puede predecir la presencia de múltiples objetos conocidos en la fotografía, como "bicicleta", "manzana", "persona", etc.

Esto es diferente a la clasificación binaria y la clasificación multiclase, donde se predice una sola etiqueta de clase para cada ejemplo.

Es común modelar tareas de clasificación de múltiples etiquetas con un modelo que predice múltiples salidas, con cada salida predicha como una distribución de probabilidad de Bernoulli. Este es esencialmente un modelo que hace múltiples predicciones de clasificación binaria para cada ejemplo.

Los algoritmos de clasificación utilizados para la clasificación binaria o multiclase no se pueden utilizar directamente para la clasificación multietiqueta. Se pueden utilizar versiones especializadas de algoritmos de clasificación estándar, las denominadas versiones de etiquetas múltiples de los algoritmos, entre las que se incluyen:

* Árboles de decisión de múltiples etiquetas
* Bosques aleatorios multietiqueta
* Aumento de gradiente de múltiples etiquetas (GradientBoosting)

Otro enfoque es utilizar un algoritmo de clasificación independiente para predecir las etiquetas de cada clase.

Podemos usar la función ``make_multilabel_classification()`` para generar un conjunto de datos sintético de clasificación multietiqueta.

### 4. Clasificación desequilibrada

La **clasificación desequilibrada** o **clasificación de clases desbalanceadas** se refiere a las tareas de clasificación en las que el número de ejemplos en cada clase se distribuye de manera desigual.

Por lo general, las tareas de clasificación desequilibrada son tareas de clasificación binaria donde la mayoría de los ejemplos en el conjunto de datos de entrenamiento pertenecen a la clase normal y una minoría de ejemplos pertenece a la clase anormal.

Ejemplos incluyen:

* Detección de fraude
* Detección de valores atípicos
* Pruebas de diagnóstico médico

Estos problemas se modelan como tareas de clasificación binaria, aunque pueden requerir técnicas especializadas.

Se pueden usar técnicas especializadas para cambiar la composición de las muestras en el conjunto de datos de entrenamiento submuestreando la clase mayoritaria o sobremuestreando la clase minoritaria.

Ejemplos incluyen:

* Sobremuestreo aleatorio o submuestreo aleatorio.
* Técnica SMOTE (*Synthetic Minority Oversampling Technique*). Ver [imbalanced-learn](https://pypi.org/project/imbalanced-learn).

Se pueden usar algoritmos de modelado especializados que presten más atención a la clase minoritaria al ajustar el modelo en el conjunto de datos de entrenamiento, como los algoritmos de aprendizaje automático sensibles a los costos.

Ejemplos incluyen:

* Regresión logística cost-sensitive.
* Árboles de decisión cost-sensitive.
* Máquinas de vectores de soporte cost-sensitive.

Para este tipo de casos se utilizan métricas de evaluación alternativas al accuracy, como por ejemplo:

* Precision
* Recall
* F1 Score (F-Measure) 

### Resumen

* La **clasificación** implica asignar una etiqueta de clase a los ejemplos de entrada.
* La **clasificación binaria** se refiere a predecir una de dos clases
* La **clasificación multiclase** implica predecir una de más de dos clases.
* La **clasificación multi-etiqueta** implica predecir una o más clases para cada ejemplo
* La **clasificación desequilibrada** se refiere a tareas de clasificación en las que la distribución de ejemplos en las clases no es igual, es decir, está desbalanceada, por ejemplo muchas observaciones de una clase y pocas de otra clase.


## Métricas de clasificación

### 1. Accuracy

```python
from sklearn.metrics import accuracy_score  
  
y_pred = [0, 2, 1, 3]  
y_true = [0, 1, 2, 3]  
  
accuracy_score(y_true, y_pred)

# output
# 0.5
```

La métrica más simple para la evaluación del modelo es el **acierto** o **exactitud** (accuracy). 

Es la relación entre el número de predicciones correctas y el número total de predicciones realizadas para un conjunto de datos.

El accuracy es útil cuando la clase de destino está bien **equilibrada**, pero no es una buena opción con clases **desequilibradas**.

Por ejemplo, un conjunto de datos con dos clases de destino que contienen 100 muestras. 98 muestras pertenecen a la clase A y 2 muestras pertenecen a la clase B en nuestros datos de entrenamiento, nuestro modelo nos daría una precisión del 98%. Es por eso que necesitamos mirar más métricas para obtener un mejor resultado.

El accuracy nos da una imagen general de cuánto podemos confiar en la predicción de nuestro modelo. Esta métrica es ciega a la diferencia entre clases y tipos de errores. Es por eso que no es lo suficientemente bueno para conjuntos de datos desequilibrados.

### 2.  Pérdida logarítmica

```python
from sklearn.metrics import log_loss  

y_true = [0, 0, 1, 1]  
y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]  

log_loss(y_true, y_pred)

# output
# 0.1738073366910675

# El primer [.9, .1] en y_pred indica una probabilidad del 90 % de que la primera muestra tenga la etiqueta 0. La pérdida logarítmica no es negativa.
```


La **pérdida logarítmica o Log Loss** se puede utilizar cuando la salida del clasificador es una probabilidad numérica en lugar de una etiqueta de clase, es decir, cuando estamos trabajando con probabilidades.

Mide la imprevisibilidad del ruido adicional que proviene del uso de un predictor en lugar de las etiquetas verdaderas.

La pérdida de registro no tiene un límite superior y existe en el rango ``[0, ∞)``. 

Cuanto más pequeña sea esta métrica significa que proporciona una mayor precisión para el clasificador.

### 3. Matriz de confusión

```python
from sklearn.metrics import confusion_matrix  
  
y_true = [1, 1, 1, 0, 0, 1, 0, 1]  
y_pred = [0, 0, 0, 0, 1, 1, 1, 0]  
print(confusion_matrix(y_true, y_pred))

# output
# [[1, 2],
#  [4, 1]]
```

Una matriz de confusión o matriz de error es una tabla que muestra el número de predicciones correctas e incorrectas realizadas por el modelo en comparación con las clasificaciones reales en el conjunto de prueba o qué tipo de errores se están cometiendo.

Esta matriz describe el desempeño de un modelo de clasificación en datos de prueba para los cuales se conocen los valores verdaderos. 

Es una matriz ``n * n``, donde n es el número de clases. Esta matriz se puede generar después de hacer predicciones sobre los datos de prueba.

Aquí, las columnas representan el recuento de clasificaciones reales en los datos de prueba, mientras que las filas representan el recuento de clasificaciones previstas realizadas por el modelo.

**Ejemplo sobre dataset diabetes:**

Tomemos un ejemplo de un problema de clasificación en el que estamos prediciendo si una persona tiene diabetes o no. Démosle una etiqueta a nuestra variable objetivo:

* 1: si una persona tiene diabetes
* 0: si una persona no tiene diabetes

Podrían ocurrir cuatro resultados posibles al realizar predicciones de clasificación:

1. **True Positives (TP)**: número de resultados que son realmente positivos y se prevé que sean positivos. 
	* Por ejemplo: en este caso, una persona en realidad tiene diabetes(1) y el modelo predijo que la persona tiene diabetes(1).

2. **Negativos verdaderos (TN)**: Número de resultados que son realmente negativos y se predicen como negativos.
	* Por ejemplo: en este caso, una persona en realidad no tiene diabetes (0) y el modelo predijo que la persona no tiene diabetes (0).

3. **Falsos positivos (FP)**: número de resultados que en realidad son negativos pero que se predijeron como positivos. Estos errores también se denominan errores de tipo 1.
	* Por ejemplo: en este caso, una persona en realidad no tiene diabetes(0) pero el modelo predijo que la persona tiene diabetes(1).

4. **Falsos negativos (FN)**: número de resultados que en realidad son positivos pero que se predijeron negativos. Estos errores también se denominan errores de tipo 2.
	* Por ejemplo: en este caso, una persona en realidad tiene diabetes(1) pero el modelo predijo que la persona no tiene diabetes(0).

Teniendo en cuenta estos nombres, el aspecto de la matriz de confusión sería el siguiente:

|            | Predicted YES        | Predicted NO         |
| ---------- | -------------------- | -------------------- |
| **Actual YES** | TRUE POSITIVES (TP)  | FALSE NEGATIVES (FN) |
| **Actual NO**  | FALSE POSITIVES (FP) | TRUE NEGATIVES (TN)  |

Siendo el ``TOTAL = TP + FN + FP + TN``

Positivo y Negativo se refiere a la predicción en sí. Verdadero y falso se refiere a la exactitud de la predicción.

A partir de los datos de la matriz de confusión se obtienen las siguientes métricas:

#### 1. Accuracy

**De todas las predicciones que hice, ¿Qué porcentaje acerté?**

``Accuracy = (TP + TN) / TOTAL``

```python
from sklearn.metrics import accuracy_score  
  
y_true = [1, 1, 1, 0, 0, 1, 0, 1]  
y_pred = [0, 0, 0, 0, 1, 1, 1, 0]  
  
accuracy_score(y_true, y_pred)

# output
# 0.25
```

Determina el porcentaje de acierto o exactitud del modelo.

Se utiliza cuando: 

* El costo de un falso positivo y un falso negativo es aproximadamente igual
* El beneficio de un Verdadero Positivo y un Verdadero Negativo es aproximadamente igual

Ejemplo: 

Predecir si una imagen es un gato o no: Un Falso Positivo o un Falso.
Los negativos tienen aproximadamente el mismo costo, asumiendo que solo queremos saber qué imágenes en nuestro álbum son gatos.

#### 2. Precision 

**¿Qué tan buenas fueron las predicciones SÍ (Predicted YES)?**

``Precision = TP / (TP + FP)``

```python
from sklearn.metrics import precision_score  
  
y_true = [1, 1, 1, 0, 0, 1, 0, 1]  
y_pred = [0, 0, 0, 0, 1, 1, 1, 0]  
  
precision_score(y_true, y_pred)

# output
# 0.3333333333333333
```

**Precision** o **Positive Predictive Value (PPV)**: Es la relación entre los verdaderos positivos y todos los positivos predichos por el modelo. 

Es útil para el conjunto de datos sesgado y desequilibrado. Cuantos más falsos positivos prediga el modelo, menor será la precisión.

Se utiliza cuando:

* El costo de un Falso Positivo es mucho mayor que el de un Falso Negativo
* El beneficio de un Verdadero Positivo es mucho mayor que el de un Verdadero Negativo

Ejemplo: 

Predecir si un correo electrónico es spam. Un Falso Positivo lo moverá en la carpeta de correo no deseado, para nunca más ser visto. Un Falso Negativo sale en la bandeja de entrada, una molestia menor.

#### 3.  Recall

* **Recall Sensitivity**
	* ¿Qué tan bueno es el modelo para predecir eventos reales SÍ (Actual YES)?
	* ``Sensivity = TP / (TP + FN)``
	
* **Recall Specificity**
	* ¿Qué tan bueno es el modelo para predecir eventos reales NO (Actual NO)?
	* ``Specificity = TN / (TN + FP)``

```python
from sklearn.metrics import recall_score  
  
y_true = [1, 1, 1, 0, 0, 1, 0, 1]  
y_pred = [0, 0, 0, 0, 1, 1, 1, 0]  

# Recall Sensitivity
recall_score(y_true, y_pred)

# Recall Specificity
recall_score(y_true, y_pred, pos_label=0)
```

**Recall** también conocido como **Sensivity** o **True Positive Rate (TPR)** es la proporción de verdaderos positivos a todos los positivos en su conjunto de datos. 

Mide la capacidad del modelo para detectar muestras positivas. Cuantos más falsos negativos prediga el modelo, menor será el recall.

A tener en cuenta con respecto a la métrica anterior de Precision:

* Precision: 
	* Depende tanto de las muestras negativas como de las positivas.
	* Considera cuando una muestra se clasifica como Positiva, pero no se preocupa por clasificar correctamente todas las muestras positivas.

* Recall: 
	* Depende solo de las muestras positivas (e independiente de las muestras negativas).
	* Se preocupa por clasificar correctamente todas las muestras positivas, pero no se preocupa si una muestra negativa se clasifica como positiva.

Se distinguen dos tipos de Recall: 


Se utiliza cuando: 

* El costo de un Falso Negativo es mucho mayor que el de un Falso Positivo
* El beneficio de un Verdadero Negativo es mucho mayor que el de un Verdadero Positivo

Ejemplo:

Predecir si un paciente tiene una enfermedad. Un falso negativo podría retrasar
tratamiento, sin embargo, un falso positivo puede conducir a más pruebas, no
tan costoso como poner una vida en juego.

#### 4. F1-score

**¿Puedo combinar Precision y Recall en una sola métrica?**

``F1 Score = 2 * ((Precision * Recall) / (Precision + Recall))``

```python
from sklearn.metrics import f1_score  
  
y_true = [1, 1, 1, 0, 0, 1, 0, 1]  
y_pred = [0, 0, 0, 0, 1, 1, 1, 0]  
  
f1_score(y_true, y_pred)

# output
# 0.25
```

El F1-score o F-measure es una métrica única que combina precision y recall. Cuanto mayor sea la puntuación de F1, mejor será el rendimiento de nuestro modelo. El rango para la puntuación F1 es ``[0,1]``.

La puntuación F1 es el promedio ponderado de precision y recall. El clasificador solo obtendrá una puntuación F alta si tanto el precision como el recall son altos. Esta métrica solo favorece a los clasificadores que tienen una precision y recall similares.

#### 5. Overall F-score
El cálculo de la puntuación F-beta sigue la misma forma que la puntuación F-1, sin embargo, también le permite decidir cómo ponderar el equilibrio entre la precisión y la recuperación utilizando el parámetro beta.

```python
from sklearn.metrics import fbeta_score  
  
y_true = [1, 1, 1, 0, 0, 1, 0, 1]  
y_pred = [0, 0, 0, 0, 1, 1, 1, 0]  
  
fbeta_score(y_true, y_pred, beta=0.5)

# output
# 0.29411764705882354
```

El F1-score es un caso generalizado del Overall F-score. 

Overall F-score tiene un factor β, que define cuánta influencia tiene la precision/recall sobre la evaluación:

* β < 1: Evaluación orientada a la precision
* β > 1: evaluación orientada al recall

F1-score es un caso generalizado en el que β es 1, lo que significa que la precision y la recall están equilibradas.

#### Métricas multi-clase

En problemas multi-clase podemos utilizar las anteriores métricas pero es necesario especificar un parámetro `average` que puede tomar los siguientes valores:

* `weighted`: calcula la puntuación para cada clase de forma independiente, pero cuando las suma usa un peso que depende del número de etiquetas verdaderas de cada clase, por lo que favorece a la clase mayoritaria.
* ``micro`` usa el número global de TP, FN, FP y calcula el F1 directamente, sin favorecer ninguna clase en particular.
* ``macro`` calcula el F1 separado por clase pero sin usar pesos para la agregación, lo que resulta en una mayor penalización cuando su modelo no funciona bien con las clases minoritarias.


#### Resumen

En scikit learn podemos obtener un resumen de todas las anteriores métricas a la vez, el siguiente código sirve para ello:


```python
from sklearn.metrics import classification_report  
  
y_true = [1, 1, 1, 0, 0, 1, 0, 1]  
y_pred = [0, 0, 0, 0, 1, 1, 1, 0]  
  
print(classification_report(y_true,y_pred))
```

Para pintar visualmente la matriz de confusión se utiliza: 

```python
from sklearn.metrics import plot_confusion_matrix  

plot_confusion_matrix(model, X_test, y_test)

# para mostrar usando porcentajes se añade el parámetro normalize:
plot_confusion_matrix(model, X_test, y_test, normalize='true')
```


### 4. ROC Curve


```python
from sklearn.metrics import RocCurveDisplay, auc, roc_curve

fpr, tpr, thresholds = roc_curve(y_test, y_pred)  
roc_auc = auc(fpr, tpr)  

display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Logistic Regression')  

display.plot()
```
![[roc-curve.png]]

Una **curva ROC (Receiver Operating Characteristic curve)** es un gráfico que muestra el rendimiento de un modelo de clasificación. 

Es una forma de visualizar la compensación entre la tasa de verdaderos positivos (TPR) y la tasa de falsos positivos (FPR) utilizando diferentes umbrales de decisión (el umbral para decidir si una predicción se etiqueta como "verdadera" o "falsa") para nuestro modelo predictivo.

Este umbral se utiliza para controlar el equilibrio entre TPR y FPR. Aumentar el umbral generalmente aumentará la precisión, pero disminuirá el recall.

Primero, veamos TPR y FPR:

* Tasa de verdaderos positivos (TPR / Sensibilidad / recall): La tasa de verdaderos positivos corresponde a la proporción de puntos de datos positivos que se consideran correctamente como positivos, para todos los puntos de datos positivos.

- Tasa de falsos positivos (FPR): la tasa de falsos positivos corresponde a la proporción de puntos de datos negativos que se consideran erróneamente como positivos, para todos los puntos de datos negativos.

Ambos tienen valores en el rango de ``[0,1]`` que se calculan en valores de umbral variables.

El clasificador perfecto tendrá:
* un valor alto de TPR
* un valor bajo de FPR

La curva característica operativa del receptor describe todos los límites de decisión posibles. La curva verde representa las posibilidades y el equilibrio entre la tasa de verdaderos positivos y la tasa de falsos positivos en diferentes puntos de decisión. Los extremos son fáciles de entender: su modelo podría predecir perezosamente 1 para TODAS las muestras y lograr una tasa de verdaderos positivos perfecta, pero también tendría una tasa de falsos positivos de 1. De manera similar, podría reducir su tasa de falsos positivos a cero prediciendo perezosamente todo. como Negativo, pero su Tasa de Verdaderos Positivos también sería cero. El valor de su modelo es su capacidad para aumentar la tasa de verdaderos positivos más rápido de lo que aumenta la tasa de falsos positivos.

![[roc-curve2.png]]

Un modelo perfecto sería una línea vertical en el eje y (100 % de verdaderos positivos, 0 % de falsos positivos). Un modelo puramente aleatorio estaría justo en la línea punteada azul (encontrar más Verdaderos Positivos significa encontrar el mismo número de Falsos Positivos).


### 5. AUC 

```python
from sklearn.metrics import roc_auc_score  

roc_auc_score(y_test, y_pred)
```

Un AUC (Área bajo la curva) o Área bajo la curva ROC, por lo que el término es la abreviatura de roc_auc.

AUC es una métrica utilizada para resumir un gráfico mediante el uso de un solo número. Se utiliza para problemas de clasificación binaria.

Nota: AUC es una función que da puntos en una curva.

AUC es igual a la probabilidad de que el clasificador clasifique un ejemplo positivo aleatorio más alto que un ejemplo negativo aleatorio.

AUC ayuda a comparar diferentes modelos ya que resume los datos de toda la curva ROC. AUC tiene un rango de ``[0,1]``. Cuanto mayor sea el valor, mejor será el rendimiento de nuestro modelo.

ROC AUC significa Característica de funcionamiento del receptor: área bajo la curva. Es una técnica para comparar el rendimiento del clasificador. En esta técnica, medimos el área bajo la curva (AUC). Un clasificador perfecto tendrá un ROC AUC igual a 1, mientras que un clasificador puramente aleatorio tendrá un ROC AUC igual a 0,5.  
  
Entonces, ROC AUC es el porcentaje del gráfico ROC que está debajo de la curva.  
  
ROC AUC es un resumen de un solo número del rendimiento del clasificador. Cuanto mayor sea el valor, mejor será el clasificador.

